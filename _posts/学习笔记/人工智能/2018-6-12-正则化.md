---
layout: blog
title: 吴恩达人工智能课程第二周笔记四：正则化 
learning:   true
category:   学习笔记
type:       学习笔记
date:       2018-6-12
tags:
- 学习笔记
- 人工智能
background-image: "/assets/nn_default_back.jpg"
---

#  吴恩达人工智能课程第二周笔记四：正则化
当你怀疑你的模型过度拟合了数据，即存在高方差的情况，通常情况下你可以通过正则化和提供更过数据来解决这个问题。

## 正则化作用原理：

### L2 正则化
在logistc回归中，我们要求损失函数的最小值，损失函数如下： 

$$ J(w, b) = \frac{1}{m}\sum^{m}_{i=1}L(\hat{y}^{(i)}, y^{(i)}) $$

其中，w是一个多维参数，b是一个实数 
在回归函数中添加**正则化**，公式就变成如下：  

$$ J(w, b) = \frac{1}{m}\sum_{i = 1}^{m}{L(\hat{y}^{(i)}, y^{(i)})} + \frac{\lambda}{2m}||w||_{2}^{2} $$

这里 \\(||w||_2^2\\)的公式为：  

$$||w||_{2}^{2} = \sum_{j = 1}^{n_x}{w_j^2} = w^Tw $$

这个方法也成为**L2正则化**，因为这里用了欧几里得法线，被称为**参数W的L2范数**

为什么只正则化w参数呢？这里不是还有一个参数b吗？为什么不加上\\(\frac{\lambda}{m}||b||_2^2\\) ?
其实也可以这么做，只是可以省略不写，因为w通常是一个高维参数矢量，已经可以表达高偏差问题。而b只是一个实数，因此w几乎涵盖所有的参数，而b不是。因此加了b也不会有什么太大的影响。

### L1 正则化
L1正则化：如果在损失函数后面添加的不是L2范数，而是L1范数即：

$$ \frac{\lambda}{m}\sum_{j = 1}^{n_x}{w_j} = \frac{\lambda}{m}||w||_1 $$

无论分母是m还是2m，它只是一个比例常量。

如果你用了L1模型，w最终会是稀疏的，也就是说w向量中有很多0，事实上L1正则化使w稀疏化，并没有降低太多的存储内存。因此L1正则化并不是为了压缩模型。

人们在训练模型的时候，越来越倾向于使用L2正则化。

λ是一个正则化参数，我们通常使用验证集来配置这个参数，尝试各种各样的数据。寻找更好的参数，我们要考虑训练集之间的权衡，把参数正常值设为较小值，这样可以避免过拟合。λ是一个需要调整的超级参数
因为上面例子谈论的是在logistic回归中的正则化情况，因此其中的w参数是一个\\([n_x, 1]\\)的向量，那么上面的W的L2范式就好理解了，如下。

$$||w||_{2}^{2} = \sum_{j = 1}^{n_x}{w_j^2} = w^Tw = [w_1, w_2,...,w_{nx}] \cdot \begin{bmatrix}
w_1 \\ 
w_2 \\
. \\
. \\
. \\
w_{nx}\\
\end{bmatrix}  = w_1^2 + w_2^2 + ... + w_{nx}^2 $$

### 在经网络里面怎么运用L2正则化？

在神经网络中我们有一个损失函数：

$$ J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]},...,w^{[L], b^{[L]}}) = \frac{1}{m}\sum_{i = 1}^{m}(\hat{y}^{(i)}, y^{(i)}) $$

正则项为：

$$ \frac{1}{m} \sum_{l = 1}^{L}||w^{[l]}||^2 $$

这个矩阵范数被定义为矩阵中所有元素的平方求和。即：

$$ ||w^{[l]}||^2 = \sum_{i = 1}^{n^{[l-1]}}{\sum_{j = 1}^{n^{[l]}}{(w_{ij}^{[l]})^2}} $$

因为对于第l层的参数w来说，它的维度是 \\( (n^{[l-1]}, n^{[l]}) \\)
该矩阵范数被称作“弗罗贝尼乌斯范数”

如何使用该范数计算梯度下降呢？
我们之前的梯度下降法公式如下：

$$
\begin{cases}
 & dw^{[l]} = \frac{\partial J}{\partial w^{[l]}} \\  
 & w^{[l]} = w^{[l]} - \alpha dw^{[1]}
\end{cases}
$$

添加正则项之后，公式应该变为如下：

$$
\begin{cases}
 & dw^{[l]} = \frac{\partial J}{\partial w^{[l]}} + \frac{\lambda}{m}{w^{[l]}} \\  
 & w^{[l]} = w^{[l]} - \alpha dw^{[1]}
\end{cases}
$$

这也是L2正则化被称为权重衰减的原因，因为最终的w的更新函数将变成下面这样：  

$$ w^{[l]} = (1 - \alpha \frac{\lambda}{m}){w^{[l]}} - \alpha \frac{\partial J}{\partial w^{[l]}}$$

该正则项说明，不论\\( w^{[l]} \\)是什么，我们都试图让它变得更小，因此L2范数正则化也被成为权重衰减